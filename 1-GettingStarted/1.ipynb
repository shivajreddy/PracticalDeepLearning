{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is it a cat model.  \n",
    "Here inputs are images.  \n",
    "Model is the Neural Net.  \n",
    "Weights are the weights in the Neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastbook\n",
    "fastbook.setup_book()\n",
    "from fastbook import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/smbp/Desktop/MyAI/PracticalDeepLearning/__PracticalDeepLearningVenv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/smbp/Desktop/MyAI/PracticalDeepLearning/__PracticalDeepLearningVenv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from fastai.vision.all import *\n",
    "path = untar_data(URLs.PETS)/'images'\n",
    "\n",
    "def is_cat(image):\n",
    "    return image[0].isupper()\n",
    "dls = ImageDataLoaders.from_name_func(\n",
    "    path, get_image_files(path), valid_pct=0.2, seed=42,\n",
    "    label_func=is_cat, item_tfms=Resize(224)\n",
    ")\n",
    "\n",
    "learn = vision_learner(dls, resnet34, metrics=error_rate)\n",
    "# learn.fine_tune(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network  \n",
    "\n",
    "Neural network, mathematically is a function, which is extremely flexible depending on its weights. The mathematical proof called **`Universal approximation theorem`** shows that this function can solve any problem to any level of accuracy in *theory*.  \n",
    "\n",
    "**`Stochastic Gradient Descent`**: Finding a new 'mechanism' for automatically updating weights for every problem.  \n",
    "\n",
    "Neural Network is a machine learning model. Neural Networks are highly flexible, and can be applied to a wide range of problems by finding the right weights.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning Jargon\n",
    "\n",
    "- The functional form of the *model* is called its *architecture* (but be careful—sometimes people use *model* as a synonym of *architecture*, so this can get confusing).\n",
    "- The *weights* are called *parameters*.\n",
    "- The *predictions* are calculated from the *independent variable*, which is the *data* not including the *labels*.\n",
    "- The *results* of the model are called *predictions*.\n",
    "- The measure of *performance* is called the *loss*.\n",
    "- The loss depends not only on the predictions, but also the correct *labels* (also known as *targets* or the *dependent variable*); e.g., \"dog\" or \"cat.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/photo1.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- weights are the parameters we tweak.  \n",
    "- Loss is the percentage that our model outputs(for validation,) when compared its results to the original results.\n",
    "\n",
    "**Limitations Inherent to Machine Learning**\n",
    "- A model cannot be created without data.\n",
    "- A model can only learn to operate on the patterns seen in the input data used to train it.\n",
    "- This learning approach only creates *predictions*, not recommended *actions*.\n",
    "- It's not enough to just have examples of input data; we need *labels* for that data too (e.g., pictures of dogs and cats aren't enough to train a model; we need a label for each one, saying which ones are dogs, and which are cats).\n",
    "\n",
    "- An AI model that will suggest a user about products is different from what organizations already do, which is they look at previous history and suggest stuff based on rank. But our model will suggest something new.\n",
    "\n",
    "Another critical insight comes from considering how a model interacts with its environment. This can create *feedback loops*, as described here:\n",
    "\n",
    "- A *predictive policing* model is created based on where arrests have been made in the past. In practice, this is not actually predicting crime, but rather predicting arrests, and is therefore partially simply reflecting biases in existing policing processes.\n",
    "- Law enforcement officers then might use that model to decide where to focus their police activity, resulting in increased arrests in those areas.\n",
    "- Data on these additional arrests would then be fed back in to retrain future versions of the model.\n",
    "\n",
    "This is a *positive feedback loop*, where the more the model is used, the more biased the data becomes, making the model even more biased, and so forth.\n",
    "\n",
    "Feedback loops can also create problems in commercial settings. For instance, a video recommendation system might be biased toward recommending content consumed by the biggest watchers of video (e.g., conspiracy theorists and extremists tend to watch more online video content than the average), resulting in those users increasing their video consumption, resulting in more of those kinds of videos being recommended. We'll consider this topic more in detail in <<chapter_ethics>>.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the third line we define a function, `is_cat`, which labels cats based on a filename rule provided by the dataset creators:\n",
    "```python\n",
    "def is_cat(x): return x[0].isupper()\n",
    "```\n",
    "\n",
    "We use that function in the fourth line, which tells fastai what kind of dataset we have and how it is structured:\n",
    "\n",
    "```python\n",
    "dls = ImageDataLoaders.from_name_func(\n",
    "    path, get_image_files(path), valid_pct=0.2, seed=42,\n",
    "    label_func=is_cat, item_tfms=Resize(224))\n",
    "```\n",
    "\n",
    "<u>There are various different classes for different kinds of deep learning datasets and problems—here we're using `ImageDataLoaders`. The first part of the class name will generally be the type of data you have, such as image, or text.</u>\n",
    "\n",
    "<u>The other important piece of information that we have to tell fastai is how to get the labels from the dataset. Computer vision datasets are normally structured in such a way that the label for an image is part of the filename, or path—most commonly the parent folder name. fastai comes with a number of standardized labeling methods, and ways to write your own. Here we're telling fastai to use the `is_cat` function we just defined.</u>\n",
    "\n",
    "Finally, we define the `Transform`s that we need. A `Transform` contains code that is applied automatically during training; fastai includes many predefined `Transform`s, and adding new ones is as simple as creating a Python function. There are two kinds: `item_tfms` are applied to each item (in this case, each item is resized to a 224-pixel square), while `batch_tfms` are applied to a *batch* of items at a time using the GPU, so they're particularly fast (we'll see many examples of these throughout this book).\n",
    "\n",
    "Why 224 pixels? This is the standard size for historical reasons (old pretrained models require this size exactly), but you can pass pretty much anything. If you increase the size, you'll often get a model with better results (since it will be able to focus on more details), but at the price of speed and memory consumption; the opposite is true if you decrease the size.\n",
    "\n",
    "Types of Supervised Learning outcomes:  \n",
    "1. **`Classification`**: A classification model is one which attempts to predict a class, or category. meaning, it predicts from a number of discrete possibilities such as dog, cat, human, tap.  \n",
    "Example: Given a photo classify it into one of the 5 categories: bird, human, ice-cream, hotdog, bird.  \n",
    "a. `Multi-classification`: cat/dog/human/coin/fish, orange/apple/pear, red/blue/yellow/green/cyan  \n",
    "b. `Binary-classification`: hotdog or not-hotdog, spam or not-spam, positive or negative  \n",
    "2. **`Regression`**: A Regression model is one which attempts to predict one or more numeric quantities, such as a temperature or a location.  \n",
    "Sometimes people use the word _regression_ to refer to a particular kind of model called a _linear regression model_; this is a bad practice.  \n",
    "Examples: Price of bitcoin, price of house. These results would be in a range  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "_PracticalDeepLearningVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
